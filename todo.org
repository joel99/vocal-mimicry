* To discuss as group
*** DONE [#B] How to implement all combos of (real/generated) and
    CLOSED: [2019-04-18 Thu 18:10]
    (positive/negative) samples for the dtor dataset classes
**** Assigned to: Anish
      Like I said my understanding of GANs comes from the original paper which
      I don't think really addresses this, so I feel like I might be missing
      some of the picture here. What I have now should be at least somewhat
      adequate though
*** DONE [#B] Talk about the content discriminator
    CLOSED: [2019-04-16 Tue 16:24]
    I'm not convinced that the content discriminator is gonna work: I had to
    come up with the arch myself, as I couldn't find anything for this online
    I think we might be better off enforcing content similarity by using a
    resnet connection across the generator :( -Anish
    * ARDA: Use Speech2Text for content loss? https://github.com/awni/speech

**** Solution: We're just gonna use the resnet way for now
* Other todo stuff
*** DONE [#A] Write generator trainer (ie whole training loop)
    CLOSED: [2019-04-18 Thu 18:10]
   * ARDA/ANISH need to collaborate on this one.
*** DONE [#A] Write dataloaders for the discriminator classes?
    CLOSED: [2019-04-15 Mon 22:11]
**** Assigned to: Anish
    I don't think there's actually much to do here, the main annoyance is just
    that the data for content/identity can be variable-length so I can't
    actually pass arround batches as tensors (so each "batch" probably has to
    consist of a single example? Not sure how that interacts with SGD though)
**** Solution: Literally just set batch_size to 1 for content/identity dtors
     and things should work
   * ARDA: We should consider padding instead.
*** DONE [#A] Validate the train_discriminator code in
    CLOSED: [2019-04-16 Tue 22:35]
    discriminators/common.py
**** Assigned to: Can't be Anish
     I found the basic structure online: would appreciate a review, since
     this is an absolutely critical function...
*** DONE [#B] Make Dataset classes, train_dtor handle data labels?
    CLOSED: [2019-04-15 Mon 22:39]
**** Assigned to: Anish
     Depends on the outcome of the discussion on "how to implement all combos"

*** DONE Implement stylevec_for_person by averaging over samples
    CLOSED: [2019-04-16 Tue 17:36]
* Simple stuff
*** TODO In isvoice_dtor decide on max vs average pooling
*** DONE In isvoice_dtor should I return scalar instead of 1d tensor?
    CLOSED: [2019-04-16 Tue 22:35]
*** TODO Decide on actual hyperparameters in the various
    get_discriminator functions
*** TODO Decide on which identity_dtor mode to use
*** DONE Apparently way that dataloaders interact w/ labels is magic?
    CLOSED: [2019-04-16 Tue 22:35]
    See https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel
*** DONE How to take care of multiple GPUs in train loop? Do we even care?
    CLOSED: [2019-04-16 Tue 17:36]

* Final todo stuff (Thursday)
** Load trained style embedder into ProjectModel
** Proper filename handling on checkpointers [Anish]
** Arda expects time in the WRONG axis

* Todo items (friday night pow-wow)
** TODO Proper filename handling on checkpoints
*** Probably done?
** TODO Axis compatibility between Anish/Joel/Arda/Alex
** TODO [#A] Remove the embedder load bypass in model.py
** TODO [#A] Remove the toy "transformer" in model.py/get_transformer
** TODO [#A] Change the starting ID in dataset.py/VCTK... back to 225!
** TODO [#A] Change the number of people in train.py back
    to 150 or whatev
** DONE Maybe automatically load the mel size from the .npy s?
   CLOSED: [2019-04-20 Sat 13:40]
** DONE Push the embedding commit to Alex's submodule
   CLOSED: [2019-04-20 Sat 14:27]
** TODO [#B] Does embedder expect time on a give axis?
** TODO [#A] Alex needs to fix following error

  warnings.warn("Bypassing loading embedder!")
No Model Found, initializing random weights
Traceback (most recent call last):
  File "train.py", line 186, in <module>
    train()
  File "train.py", line 146, in train
    args.mel_root)
  File "/home/anish/Code/vocal-mimicry/dataset.py", line 115, in __init__
    self._calculate_person_stylevecs()
  File "/home/anish/Code/vocal-mimicry/dataset.py", line 133, in _calculate_person_stylevecs
    sample_stylevecs[sid] = torch.from_numpy(self.embedder(mel))
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/anish/Code/vocal-mimicry/embedding/model.py", line 209, in forward
    x = self.model.fc(x)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 67, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/functional.py", line 1352, in linear
    ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())
RuntimeError: size mismatch, m1: [1 x 13824], m2: [1024 x 512] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940
*** dataset.py/calculate_person_stylevecs
** TODO [#A] Revert embedder in train.py (creation of VCTK Wrapper) to
    actually do something!
** DONE [#A] Anish, fix the following error
   CLOSED: [2019-04-20 Sat 13:27]

(ml) anish@anish-ubuntu:~/Code/vocal-mimicry$ python train.py --mel-size 30 --num-epochs 10 --dset-num-people 2 --dset-num-samples 5 --num-batches-dtor-isvoice 10 --batch-size-dtor-isvoice 10 --num-batches-tform 10 --batch-size-tform 10
/home/anish/Code/vocal-mimicry/model.py:54: UserWarning: Bypassing loading embedder!
  warnings.warn("Bypassing loading embedder!")
No Model Found, initializing random weights
Started Training at 2019-04-20 02:13:00.082727
Traceback (most recent call last):
  File "train.py", line 192, in <module>
    train()
  File "train.py", line 181, in train
    args.num_batches_dtor_isvoice)
  File "/home/anish/Code/vocal-mimicry/discriminators/common.py", line 207, in train_dtor
    predictions = dtor(data, lengths).view(-1)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/anish/Code/vocal-mimicry/discriminators/isvoice_dtor.py", line 104, in forward
    return torch.sigmoid(self.fc_layer(flattened))
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 67, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/anish/venv/ml/lib/python3.6/site-packages/torch/nn/functional.py", line 1352, in linear
    ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())
RuntimeError: size mismatch, m1: [5 x 1188], m2: [54 x 1024] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940
** TODO [#A]  Revert the isvoice_dtor to the actual architecture!
** TODO [#A] In train.py, set default tensor type to cuda.floattensor?
** TODO [#A] In new_train.py, I'm slicing None at the right index?
