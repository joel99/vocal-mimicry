<!DOCTYPE html>
<!-- saved from url=(0039)https://joel99.github.io/vocal-mimicry/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>One-Shot Vocal Imitation | Data-Efficient Voice Imitation</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="Data-Efficient Voice Imitation">
<meta property="og:locale" content="en_US">
<meta name="description" content="Investigating Vocal Style Transfer in the Few Shot Setting">
<meta property="og:description" content="Investigating Vocal Style Transfer in the Few Shot Setting">
<link rel="canonical" href="https://joel99.github.io/vocal-mimicry/">
<meta property="og:url" content="https://joel99.github.io/vocal-mimicry/">
<meta property="og:site_name" content="Data-Efficient Voice Imitation">
<script type="application/ld+json">
{"description":"Investigating Vocal Style Transfer in the Few Shot Setting","@type":"WebSite","url":"https://joel99.github.io/vocal-mimicry/","name":"Data-Efficient Voice Imitation","headline":"One-Shot Vocal Imitation","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="assets/style.css">
  <style></style></head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Data-Efficient Voice Imitation</h1>
      <h2 class="project-tagline">Investigating Vocal Style Transfer in the Few Shot Setting</h2>
      
        <a href="https://github.com/joel99/vocal-mimicry" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
      <h1 id="one-shot-vocal-imitation">One-Shot Vocal Imitation</h1>
<p>CS7643 Final Project by Alex Le, Anish Moorthy, Arda Pekis, Joel Ye</p>

<h2 id="introduction">Introduction</h2>
<p>
    For our project, we attempted to perform style transfer on voices: given an audio clip from speaker A, we attempt to produce another audio clip with the same content (volume modulation, timing, etc), but which sounds like it was spoken by speaker B.
</p>

<p>
    Voice cloning has been a topic of interest for many years now, but up until recently many models have required an abundance of source / target training data for a single sample [1] (on the order of 5-10 hours of training data). In recent years, much progress has been made: to our knowledge, the current SOTA model is [3] which can indeed produce good results with little data. However even this model uses text as an intermediate representation, an embedding which ignores many important aspects of spoken communication such as intonation and timing.
</p>

<p>
    To surpass these limitations, we aimed to perform data-efficient style transfer without the use of test as an intermediate representation. Such technology would have many real-world applications, especially in the modern entertainment industry where it could be used to preserve iconic voices for use in future media or to adapt an actors voice to fit the role of the film, removing the need for actors to spend months of training to building or losing accents. This technology could also be useful in the human interactive field for personalizing a user’s Siri, Google Assistant, Alexa or Cortana. Having a similar accent could improve the performance and usefulness of these applications.
</p>

<h2 id="approach">Approach</h2>
<p>
    For our data-embedding, we preprocess into log-mel spectrograms, a 2d-representation of audio with time on one axis and frequency on the other. We chose this embedding for two key reasons. First, log-mel spectrograms are a standard embedding for working with audio data, and so there exist sophisticated tools for converting them back to audio: we use Nvidia’s open-source WaveGlow [2] network for this exact purpose. We verified that the existing implementation provided produces convincing reconstructions of our mel embeddings of audio clips. Second, they allow us to use (and indeed, are a natural target for) Convolutional Neural Networks throughout our architecture. All of our code is implemented in the Pytorch framework.
</p>

<figure>
  <img src="./assets/figures/mel-example.png" alt="Mel">
  <figcaption style="text-align: center">Example of Mel-Spectrogram [4]</figcaption>
</figure>

<p>
    We chose a generative model architecture, illustrated below, designed to counter anticipated problems. The Transformer Network (corresponding to the generator) attempts to, given a source audio file and target style vector, output another audio file which has low distance from the target style vector. Content similarity is enforced via the residual connection across the transformer network, which biases the transformer towards producing something similar to the input mel: the source of the content information which we wish to preserve. To prevent the model from simply producing noise which is overfit to the style embedder, we include two discriminators: one for determining whether the output sounds like a voice, and another for determining whether it is an original or transformed voice.
</p>

<p><img src="assets/figures/model-arch.png" alt="Arch"></p>

<p>
    We believed that this model was suited to our objective because it offered a clear target to be made data-efficient: the style embedder. Using small amounts of source and target data (as was our goal), it would have been difficult to train the model end-to-end from scratch. However using a large dataset such as VoxCeleb to train the Style Embedder to efficiently estimate future audio samples would make the problem easier: assuming a “reliable” style embedder, the Transformer and Discriminator networks do not have to be architected around data efficiency and can also be trained using large datasets.
</p>

<p>
    In summary, the style embedder and waveglow reconstruction modules exist independently, and the transformer and discriminators are the bulk of the model to be trained. (TODO A note about hyperparameters and optimizers @Arda)
</p>

<h3 id="discriminators">Discriminators</h3>
<p>
    For the IsVoice? discriminator, the 2d data allows us to employ CNNs, which have been demonstrated to be suited for audio classification [5][6]. The following architecture is adapted from [6] to accommodate variable time-length data via the global max-pool across time.
</p>

<p><img src="assets/figures/isvoice-arch.png" alt="discriminator"></p>

<p>For the OriginalVoice? discriminator, we decided to use </p>

</p>
<p>
<img style="display: block;margin:auto;" src="assets/eq/atan.png" alt="atan">
</p>
<p>
<img src="assets/eq/sout.png" alt="sout" style='height: 22px'>
is the style embedding of the transformer’s output. We considered a Siamese Network instead, but decided against it due to (a) potential to overfit and (b) added model parameters which would inflate complexity and training times.
</p>

<p>
    As described in Goodfellow’s original paper on GANs, both discriminators output a probability that their input belongs to the True (as opposed to Generated) distribution. We  train the IsVoice? Discriminator via minimizing the binary cross-entropy between its prediction and  the true label (either real or generated) of an input sample.
</p>

<p>After a batch of discriminator training, the Transformer is trained to minimize </p>
<img style="display: block;margin:auto;" src="assets/eq/gan.png" alt="gan">
<p>
  (in our scheme, “1” is the label for “real” data). Thus, the Generator attempts to produce voices which sound “untransformed” and “like a real voice” as defined by the discriminators.</p>

<h3 id="style-embedder">Style Embedder</h3>
<p>The style embedder is adapted from the Deep Speaker model [8], which learns utterance-level speaker embeddings to classify different speakers. We re-purpose these embeddings as style vectors to be used by the transformer. The style embedder is built with a Residual Network with average pooling over frame-level input, affine layer, and length normalization layer at the end as shown in figure below.
<img src="assets/figures/embedder_arch_pic.png" alt="embedder"></p>

<p>The embedder is pre-trained in a 2 step process, Cross Entropy Loss + Triplet Loss and just Triplet Loss. Having an initial softmax training process results in more stable convergences and produces better results. Triplet loss with cosine similarity is designed to captured the differences in different people’s voice, but similarities in one person’s voice with different dialogues as shown below. </p>
<p>
  <img style="display: block;margin:auto;" src="assets/eq/triplet.png" alt="triplet_eq">
</p>
<p>
<img style="display: block;margin:auto;" src="assets/figures/triplet_loss.png" alt="triplet">
</p>

<h3 id="transformer">Transformer</h3>
<p>
We adapt the transformer-xl model used in Dai et al. to handle the sequence to sequence task of transforming audio mel spectrograms rather than the original language modeling task[10]. Transformer-XL combines relative positional attention with an RNN to achieve longer term time dependencies than typical transformers as well as significantly speeding up inference, both of which are important traits for audio models.
</p>
<p>
<img style="display: block;margin:auto;", src="assets/figures/transformer.png" alt="transformer">
</p>

<h2 id="experiments-and-results">Experiments and Results</h2>
<p>
    Vocal imitation, as a type of style transfer, is difficult to evaluate objectively. A typical strategy used for style transfer results is Mean Opinion Survey, where opinions on task success are gathered and averaged. However, assessment of our results clearly showed that our model learned almost nothing, and so we did not gather these quantitative scores. Example model input/outputs are provided below.
</p>

<p>
For our transformer model, we used an inner dimension of 64, 4 16-dimensional attention heads per layer, a 512-dimensional style representation and 4 layers. We would have preferred using larger dimensionalities but the memory usage of the transformer model was prohibitive.
</p>

<p>
    Our main experiment looked at the effects of the residual connection across the transformer network, as shown in our architecture map. Originally, this connection was not included in the architecture: however, without the residual connection our model produced non-random but meaningless noise (omitted as it is painful to listen to). The final validation loss, evaluated relative to the discriminators, was 1.56, which was an improvement over the initial loss of 2.93. After adding and retraining with the residual connection, results improved somewhat in that the network produced distorted versions of the source input [LINK]. The transformer’s validation loss relative to the discriminators was 3.08, initially 3.27. However, even with the residual connection we found the target style vector has little semantic effect on the output of the transformer: that is, the model was unable to learn the “style transfer” function that it was designed to.
</p>

<p>Realization of poor results indicate a few directions we could try to improve. A more rigorous assessment of each component could reveal unexpected behavior that would bring down the whole models performance.
@Arda send some further insight. Include mel images.</p>


<h2 id="analysis">Analysis</h2>
<p>
    Given the results above, it is highly likely that our model’s failure is attributable to a weak and/or noisy style loss signal. We have shown that our model tends to learn noisy distortions of the input clips: since the Style Embedder is trained on data which does not contain these distortions, it may not generalize well to the progressively-more distorted inputs we give it.  This would make the style loss extremely noisy, which would in turn explain our inability to learn anything meaningful. The better loss of the no-residual transformer suggests that the style loss is misbehaving outside of its training data regime, thus giving erroneously low loss values. We believe that this generalization error of the style embedder has at least something to do with our results, since the DeepSpeaker architecture is based on ConvNets and numerous sources, not least our own Assignment 2 [9], have demonstrated that even small perturbations in inputs may confuse CNNs.
</p>
<p>
    However even assuming that the style embedder generalized well, however, it is possible that the OriginalVoice discriminator needed more representational power than given by the simple arctan-of-norm function. In retrospect, even a simple multi-layer perceptron network might have been useful here to allow the discriminator function to capture, for example, differing importance of features in the style vectors, or better enabling the network to adapt to the adversarial samples generated by the transformer.
</p>
<p>
    At the very least, these poor results indicate further directions for modification and exploration which could yield improvement.
</p>
<p>
    <ol>
        <li>
         The most important task would be to verify the robustness of the Style Embedder to noise: say, by adding noise to samples and measuring the resultant perturbations in the style vectors. If this is revealed to be a problem, then the next step would be to retrain the Style Embedder on a noise-augmented dataset, potentially including adversarial noise via FGSM [11], so that it can better handle the output the transformer produces.
        </li>
        <li>
            Furthermore, our goal was to produce realistic-sounding voices and the RealVoice discriminator could not prevent noticeable distortions in the model outputs. Given that the RealVoice architecture (illustrated above) without the global max-pool has been successfully applied to audio classification before, our max-pool probably causes the degradation simply because it throws away so much information. Replacing the max-pool with a recurrent neural network would have been a more appropriate way to synthesize information across time.

        </li>
    </ol>
</p>

<h2 id="samples">Samples</h2>
<p>Provided below are a random selection of clips generated by our system. No residual connection results omitted (they are painful to listen to!)</p>

<h4 id="voice-reconstruction-waveglow">Voice reconstruction (WaveGlow):</h4>
<p>
  <a href="https://joel99.github.io/vocal-mimicry/gh-pages/assets/raw/p300_001.wav">Original</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p300_001.wav" type="audio/wav">
    </audio>
</p>
<p>
<a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/synth_reconstruct/p300_001_synthesis.wav">Reconstructed</a>
<audio controls>
  <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/synth_reconstruct/p300_001_synthesis.wav" type="audio/wav">
  </audio>
</p>
  <h4 id="residual-connection-enabled">Residual connection enabled:</h4>
  <h5 id="example-1">Example 1</h5>
<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p226_002.wav">Source Content</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p226_002.wav" type="audio/wav">
  </audio>
</p>

<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p225_001.wav">Source Style</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p225_001.wav" type="audio/wav">
  </audio>
</p>

<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/waves_with_res/new_p226_002_p225_001_synthesis.wav">Transferred</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/waves_with_res/new_p226_002_p225_001_synthesis.wav" type="audio/wav">
  </audio>
</p>

<h5 id="example-1">Example 1</h5>
<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p231_002.wav">Source Content</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p231_002.wav" type="audio/wav">
  </audio>
</p>

<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p232_003.wav">Source Style</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/raw/p232_003.wav" type="audio/wav">
  </audio>
</p>

<p>
  <a href="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/waves_with_res/new_p231_002_p232_003_synthesis.wav">Transferred</a>
  <audio controls>
      <source src="https://raw.githubusercontent.com/joel99/vocal-mimicry/gh-pages/assets/waves_with_res/new_p231_002_p232_003_synthesis.wav" type="audio/wav">
  </audio>
</p>

<p>Full samples can be found <a href="https://drive.google.com/drive/folders/1DkMnvIJAAZfhHzJqI3e2uVBKnalxsZpB?usp=sharing">here</a>
Naming convention is “target_source.wav” where target is the clip’s whose audio . Original VCTK audio files are also provided.</p>

<h2 id="references">References</h2>
<ol>
  <li>Sercan Arik et al. “Deep Voice: Real-time Neural Text-to-Speech” (Feb 2017)</li>
  <li>Ryan Prenger, Rafael Valle, Bryan Catanzaro. “WaveGlow: A Flow-based Generative Network for Speech Synthesis” (Oct. 2018)</li>
  <li>Sercan Arik et al. “Neural Voice Cloning with a Few Samples” (Feb 2018)</li>
  <li>Librosa librosa.feature.melspectrogram Documentation</li>
  <li>Hershey et al. “CNN Architectures for Large-Scale Audio Classification” (2017)</li>
  <li>Hossein Salehghaffari. “Speaker Verification using Convolutional Neural Networks” (Mar 2018)</li>
  <li>Ian Goodfellow et al. “Generative Adversarial Networks” (Jun 2014)</li>
  <li>Chao Li et al. “Deep Speaker: an End-to-End Neural Speaker Embedding System” (2017)</li>
  <li>Gatech CS7643 Spring 2019, “Homework 2 (Coding)”</li>
  <li>Dai, Zihang, et al. "Transformer-xl: Attentive language models beyond a fixed-length context." arXiv preprint arXiv:1901.02860 (2019).</li> 
  <li>Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).</li>
</ol>



      <footer class="site-footer">

          <span class="site-footer-owner"><a href="https://github.com/joel99/vocal-mimicry">vocal-mimicry</a> is maintained by <a href="https://github.com/joel99">joel99</a>. Find other contributors on the GitHub.</span>
      </footer>
    </section>




</body></html>
